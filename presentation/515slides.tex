\documentclass[12pt]{beamer}
\usetheme{CambridgeUS}
\usecolortheme{dolphin}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{multimedia}
\usepackage[mathscr]{euscript}
\usepackage{xcolor}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{setspace}
\usepackage[overlay,absolute]{textpos}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Tau}{\mathcal{T}}
\newcommand{\AMz}{\underset{z}{arg\, min}}
\newcommand{\prox}{\text{prox}}

\newcommand{\worm}{WWWWWoooooooRRRRRmmmmmm}

\author[Rudy,Maass,Molloy,Mueller]{Samuel Rudy, Kelsey Maass, Riley Molloy, and Kevin Mueller}
\title[Denoising and Deblurring]{Image Denoising and Deblurring \\ \normalsize Applied Math 515 Final Project}
\setbeamercovered{transparent} 
%\institute{University of Washington} 
\date{March 12 2016} 
\subject{Amath 575} 

\iffalse
To do:
-Should we call it an objective or loss function?  Need to be consistent.
-Add in plot comparing methods
-Image assumptions and regularizers slide needs to be finished
-How do we compute prox_{\| W \cdot \|_1} ? Should add in on L1 wavelet reg slide
-Finish conclusions
-Add in sources and citations throughout presentation
\fi

\begin{document}

% Title Page
\begin{frame}
\titlepage
\end{frame}

% Contents
\begin{frame}{Contents}
\tableofcontents
\end{frame}

% MOTIVATION
\section{Motivation}

% Cameraman example
\begin{frame}{Image Denoising and Deblurring}
\begin{center}
\includegraphics[scale=0.55]{../figures/bad_image.png}
\hspace{1 cm}
\includegraphics[scale=0.55]{../figures/good_image.png} 
\end{center}
\end{frame}

% Problem Formulation
\begin{frame}{Mathematical Formulation}
\begin{center}
\vspace{-3ex}
\includegraphics[scale=0.5]{../figures/linearModel}
\end{center}

\vspace{-3ex}
\begin{itemize}
\item Blur: $Ax$ is a discrete convolution of the true image with a Gaussian kernel (reflexive boundary conditions).
\item Noise: $w$ is noise drawn from a Gaussian or Student's t distribution. 
\end{itemize}
\end{frame}

% Naive Solution
\begin{frame}{Naive Solution: $x = A^{-1}b$}
\begin{figure}
\centering
\includegraphics[scale=0.2]{../figures/fig1} \,
\includegraphics[scale=0.2]{../figures/fig2} \,
\includegraphics[scale=0.2]{../figures/fig3} \\
\small{\hspace{1em} True image \hspace{4em} Blurred image \hspace{3em} Recovered image}
\end{figure}
\end{frame}

\begin{frame}{Naive Solution: $x = A^{-1}(b-w)$}
\begin{figure}
\centering
\includegraphics[scale=0.2]{../figures/fig1} \,
\includegraphics[scale=0.2]{../figures/fig4} \,
\includegraphics[scale=0.2]{../figures/fig5} \\
\small{\hspace{1em} True image \hspace{2em} Blurred and noisy image \hspace{1em} Recovered image}
\end{figure}
\end{frame}

% OBJECTIVE FUNCTIONS
\section{De(noise/blur)ing Objective Functions}

% General Objective Function
\begin{frame}
Since the blur operator is ill-conditioned, a better approach is to minimize a regularized objective function.
\begin{block}{General Objective Function}
\[ L_b(x) \hspace{0.5em} = \hspace{0.5em} \underbrace{f(Ax-b)}_{\text{Fidelity term}} \hspace{0.5em} + \underbrace{\lambda R(x)}_{\text{Regularization}} \]
\end{block}

\begin{columns}[T]
	\begin{column}{0.45\linewidth}
		\begin{block}{Fidelity Terms} \vspace{-2.5ex}
			\[ f = \left\{ \begin{matrix*}[l] 
			\| \cdot \|_F^2 \\[1ex] h_{\gamma}( \cdot ) \\[1ex] \gamma^{-1} \log(\cosh(\gamma \cdot)) 
			\end{matrix*} \right. \]
		\end{block}
	\end{column}
	\begin{column}{0.45\linewidth}
		\begin{block}{Regularization Terms}
			\[ R = \left\{ \begin{matrix*}[l]
			\| Wx \|_1 \\[1ex] TV(x)
			\end{matrix*} \right. \]
		\end{block}
	\end{column}
\end{columns}
\end{frame}

% Fidelity Term
\begin{frame}{Fidelity Term Penalty Functions}

The fidelity term $f(Ax-b)$ measures how well our results comply with the linear blurring model. Depending upon the type of noise present in the observed image, the choice of penalty function may influence the efficacy of our deblurring/denoising procedure. 

\begin{columns}[T]
	\begin{column}{0.45\linewidth}
		\begin{block}{Gaussian Noise}
		\quad \includegraphics[scale=0.15]{../figures/gaussian_noise.png} \hspace{1.5em}
		\includegraphics[scale=0.1]{../figures/quadratic} \\
		{\footnotesize Due to the lack of outliers, the quadratic penalty is sufficient} \\[1ex]
		\hspace{3em} $f(z) = \frac{1}{2} \| z \|^2$
		\end{block}
	\end{column}
	\begin{column}{0.45\linewidth}
		\begin{block}{Student's t Noise}
		\quad \includegraphics[scale=0.15]{../figures/student_t_noise.png} \hspace{1.5em}
		\includegraphics[scale=0.1]{../figures/huber} \\
		{\footnotesize Huber penalty preferred since it is more robust to heavy-tailed noise} \\[1ex]
		$f(z) = \min_y \frac{1}{2} \| z-y \|^2 + \gamma \| y \|_1$
		\end{block}
	\end{column}
\end{columns}

\end{frame}

% Regularizers: Wavelet
\begin{frame}{L1 Wavelet Regularization}

\vspace{-4ex}
\[ R(x) = \| W x \|_1 \]

\vspace{2ex}
\begin{minipage}[T]{0.45\textwidth}
	\begin{center}
	\includegraphics[width=1\textwidth]{../figures/haarTrans.pdf} \\
	Haar Transform (2 Levels)
	\end{center}
\end{minipage}
\begin{minipage}[T]{0.5\textwidth}
Images are often sparse in wavelet domains, so the L1 wavelet regularizer can be used to encourage this property in our recovered image. \\

For our examples, we let $W$ be the orthogonal 2D Haar or Daubechies wavelet transform using 5 levels. \\[2ex]
\end{minipage}

\end{frame}

% Regularizers: TV
\begin{frame}{Total Variation Regularization}

\vspace{-4ex}
\[ R(x) = TV(x) \]

\vspace{2ex}
\begin{minipage}[T]{0.45\textwidth}
	\begin{center}
	\includegraphics[width=1\textwidth]{../figures/diff.pdf} \\
	
	\end{center}
\end{minipage}
\begin{minipage}[T]{0.5\textwidth}
The TV norm penalizes a finite difference representation of a derivative, assuming small variation in pixels nearby \\[4ex]
\end{minipage}
\tiny $$ TV_{l_1}(x) = \sum_{i=1}^{m-1} \sum_{j=1}^{n-1} \left( \abs{x_{i,j}  - x_{i+1,j} } + \abs{ x_{i,j} - x_{i,j+1}  } \right) + \sum_{i=1}^{m-1} \abs{ x_{i,n} - x_{i+1,n} } + \sum_{j=1}^{n-1} \abs{ x_{m,j} - x_{m,j+1 } }$$
\end{frame}

% OPTIMIZATION WITH L1 WAVELET REGULARIZATION
\section{Optimization with L1 Wavelet Regularization}

% Wavelet Objective Function
\begin{frame}{L1 Wavelet Regularization}
\begin{block}{Objective Function}
\[ L_b(x) = f(Ax-b) + \lambda \| Wx \|_1 \vspace{1ex} \]
\end{block}

\vspace{2ex}
\begin{block}{Proximal Gradient Step}
\begin{align*}
x^{k+1} &= \prox_{\alpha^{-1}\lambda \| W \cdot \|_1} \Big(x^k - \alpha^{-1} A^T\nabla f (Ax^k - b)\Big) \\
&= W^* \prox_{\alpha^{-1}\lambda \| \cdot \|_1} \Big(W\big(x^k - \alpha^{-1} A^T\nabla f (Ax^k - b)\big)  \Big)  
\end{align*}
\end{block}
\end{frame}

% FISTA algorithm
\begin{frame}{FISTA L1 Wavelet Regularization}
\begin{center}
\begin{minipage}[T]{0.7\textwidth}
\textbf{FISTA$(b,f,\lambda)$}
\begin{algorithmic}
\State $y^1 = x^0 = b; \, t^1 = 1$
\State $\alpha \geq Lip(\nabla f)$
\For {$k = 1:N$}
	\State $u^k = y^k - \alpha^{-1} A^T \nabla f(Ay^k-b)$ \vspace{1ex}
	\State $x^k = W^*\text{sgn}(Wu^k)\max(0,|Wu^k|-\alpha^{-1}\lambda)$ \vspace{1ex}
	\State $t^{k+1} = \frac{1+\sqrt{1+4t_2^k}}{2}$
	\State $y^{k+1} = x^k + \left(\frac{t^k-1}{t^{k+1}}\right) (x^k - x^{k-1})$
\EndFor\\
\Return $x^N$
\end{algorithmic}
\end{minipage}
\end{center}
\end{frame}

% Note on blur operator
\begin{frame}{Note on Blur Operators}
For our assumed reflexive boundary conditions, the matrix $A$ is a Kronecker product of Toeplitz-plus-Hankel matrices which can be diagonalized by the discrete cosine transform
\[ A = C^T\Lambda C, \]
where the eigenvalues are determined by the blur kernel. This means that to blur an image, we don't actually have to construct $A$:
\[ Ax = {\tt idct2}\big(\Lambda {\tt dct2}(x)\big) \]
\end{frame}

\begin{frame}{Note on Blur Operators}
When $f = \| Ax-b \|^2$ we can exploit the fact that $C$ is unitary:

\begin{align*}
f(Ax-b) &= \| Ax - b \|^2 \\
&= \| C^T\Lambda C x - b \|^2 \\
&= \| \Lambda Cx - Cb \|^2 \\
&= \| \Lambda \hat{x} - \hat{b} \|^2
\end{align*}
\end{frame}

% Results: Gaussian Noise (Haar)
\begin{frame}{Results: Gaussian Noise (Haar)}
\begin{center}
\vspace{-3 mm}
\includegraphics[width = 0.8\textwidth]{../figures/waveletGaussH.pdf} 
\end{center}

\begin{textblock}{5}(1.5,3.6)
\rotatebox{90}{Original Image}
\end{textblock}

\begin{textblock}{5}(1.5,9.6)
\rotatebox{90}{Frobenius Loss}
\end{textblock}

\begin{textblock}{5}(14.1,3.2)
\rotatebox{90}{Blurred and Noisy}
\end{textblock}

\begin{textblock}{5}(14.1,10)
\rotatebox{90}{Huber Loss}
\end{textblock}
\end{frame}

% Results: Gaussian Noise (Daubechies)
\begin{frame}{Results: Gaussian Noise (Daubechies)}
\begin{center}
\vspace{-3 mm}
\includegraphics[width = 0.8\textwidth]{../figures/waveletGaussD.pdf} 
\end{center}

\begin{textblock}{5}(1.5,3.6)
\rotatebox{90}{Original Image}
\end{textblock}

\begin{textblock}{5}(1.5,9.6)
\rotatebox{90}{Frobenius Loss}
\end{textblock}

\begin{textblock}{5}(14.1,3.2)
\rotatebox{90}{Blurred and Noisy}
\end{textblock}

\begin{textblock}{5}(14.1,10)
\rotatebox{90}{Huber Loss}
\end{textblock}
\end{frame}

% Results: Student's t Noise (Haar)
\begin{frame}{Results: Student's t Noise (Haar)}
\begin{center}
\vspace{-3 mm}
\includegraphics[width = 0.8\textwidth]{../figures/waveletStudentH.pdf} 
\end{center}

\begin{textblock}{5}(1.5,3.6)
\rotatebox{90}{Original Image}
\end{textblock}

\begin{textblock}{5}(1.5,9.6)
\rotatebox{90}{Frobenius Loss}
\end{textblock}

\begin{textblock}{5}(14.1,3.2)
\rotatebox{90}{Blurred and Noisy}
\end{textblock}

\begin{textblock}{5}(14.1,10)
\rotatebox{90}{Huber Loss}
\end{textblock}
\end{frame}

% Results: Student's t Noise (Daubechies)
\begin{frame}{Results: Student's t Noise (Daubechies)}
\begin{center}
\vspace{-3 mm}
\includegraphics[width = 0.8\textwidth]{../figures/waveletStudentD.pdf} 
\end{center}

\begin{textblock}{5}(1.5,3.6)
\rotatebox{90}{Original Image}
\end{textblock}

\begin{textblock}{5}(1.5,9.6)
\rotatebox{90}{Frobenius Loss}
\end{textblock}

\begin{textblock}{5}(14.1,3.2)
\rotatebox{90}{Blurred and Noisy}
\end{textblock}

\begin{textblock}{5}(14.1,10)
\rotatebox{90}{Huber Loss}
\end{textblock}
\end{frame}

% Choosing Lambda
\begin{frame}{Choosing $\lambda$}
\begin{center}
\includegraphics[width=0.9\textwidth]{../figures/wavLam.pdf}
\end{center}

\begin{textblock}{5}(0.9,6)
\rotatebox{90}{Haar}
\end{textblock}

\begin{textblock}{5}(0.9,9.5)
\rotatebox{90}{Daubechies}
\end{textblock}

\begin{textblock}{5}(2.5,13.3)
$\lambda = 10^{-4}$
\end{textblock}

\begin{textblock}{5}(6.9,13.3)
$\lambda = 10^{-3}$
\end{textblock}

\begin{textblock}{5}(11.3,13.3)
$\lambda = 10^{-2}$
\end{textblock}

\end{frame}

% OPTIMIZATION WITH TOTAL VARIATION REGULARIZATION
\section{Optimization with Total Variation Regularization}

\begin{frame}{Total Variation Regularization}

\begin{exampleblock}{Objective Function}
$$
L_b(x) = f(Ax-b) + \lambda \mathrm{TV}(x) + \delta(x | [0,1])
$$
\end{exampleblock}

\begin{exampleblock}{Proximal Gradient Step}
\begin{align*}
x^{k+1} &= \prox_{\alpha^{-1}(\lambda \|\cdot \|_{TV} + \delta_{[0,1]})} (\underbrace{x^k - \alpha^{-1} A^T\nabla f (Ax^k - b)}_{u^k}) \\
&= \AMz \left( \|u^k - z\|_F^2 + \alpha^{-1}\lambda \|z\|_{TV} + \delta(z | [0,1]) \right) \\
&= P_{[0,1]}  \left( \AMz \left( \|u^k - z\|_F^2 + \alpha^{-1}\lambda \|z\|_{TV} \right) \right)
\end{align*}
\end{exampleblock}

\end{frame}

\begin{frame}{Dual Form of Total Variation}

\begin{exampleblock}{A Few Definitions}
\begin{itemize}

\item $\mathcal{P} = \{ (p,q) \in \R^{(m-1) \times n} \times \R^{ m \times (n-1) } :  \abs{p_{i,j} } \le 1, \abs{p_{ i,j } } \le 1 \},$

\item $ \mathcal{L} : \R^{(m-1) \times n} \times \R^{ m \times (n-1) } \rightarrow \R^{m \times n}$ such that

$$ \mathcal{L}(p,q)_{i,j} = p_{i,j} + q_{i,j} - p_{ i-1,j } - q_{ i,j-1 }$$ for $i = 1,\dots,m$, $j = 1,\dots,n$, and 

$ p_{0,j} = p_{m,j} = q_{i,0} = q_{i,n} = 0.$ 
\end{itemize}
\end{exampleblock}

\begin{exampleblock}{Total Variation}
$ \mathrm{TV}(x) = max_{p,q \in \mathcal{P}} T(x, p,q) \implies T(x,p,q) = \mathrm{Tr} ( \mathcal{L}(p,q)^T x ).$
\end{exampleblock}

\end{frame}

\begin{frame}{Dual Form of TV Denoising with $\|\cdot \|_F^2$}

The problem:
\begin{align*}
\min_{x \in [0,1]} &\norm{x - b}_F^2 + 2 \lambda \mathrm{TV}(x) \\
\min_{x \in [0,1]} \max_{(p,q,) \in \mathcal{P}} &\norm{x - b}_F^2 + 2 \lambda \mathrm{Tr} ( \mathcal{L}(p,q)^T x )
\end{align*}
Dual problem: 
\begin{align*}
\min_{(p,q) \in \mathcal{P} } & \underbrace{- \norm{ H_{[0,1]} (b- \lambda \mathcal{L}(p,q) ) }_F^2 + \norm{ b - \lambda \mathcal{L}(p,q) }_F^2}_{h(p,q)} \\
H_{[0,1]}(\cdot ) & = (I - P_{[0,1]})(\cdot )
\end{align*}

Optimality conditions: 
$$ x = P_C(b - \lambda \mathcal{L}(p,q) )$$

\end{frame}

\begin{frame}{Optimization of Dual Form}

\begin{exampleblock}{Problem Statement}
\vspace{-5 mm}
\begin{align*}
&\underset{(p,q) \in \mathcal{P} }{\text{min}}  \left\{  \norm{b - \lambda \mathcal{L} (p,q)}_F^2 - \norm{(I-P_{[0,1]}) (b - \lambda \mathcal{L} (p,q))}_F^2   \right\}\\
&\underset{(p,q) }{\text{min}}  \left\{  \underbrace{\norm{b - \lambda \mathcal{L} (p,q)}_F^2 - \norm{(I-P_{[0,1]}) (b - \lambda \mathcal{L} (p,q))}_F^2}_{= h(p,q)} + \delta((p,q) | \mathcal{P})   \right\}
\end{align*}
\end{exampleblock}

\begin{exampleblock}{}
\begin{align*}
&\nabla h (p,q) = -2\lambda \mathcal{L}^TP_{[0,1]} (b - \lambda \mathcal{L}(p,q)) \\
&\text{Lipschitz with constant } \leq 16\lambda^2\\
&\Rightarrow \text{ Use projected gradient}
\end{align*}
\end{exampleblock}

\end{frame}

\begin{frame}{Optimization in Dual Form}

\begin{exampleblock}{Projection onto $\mathcal{P}$}
Recal $\mathcal{P} = (p,q) \in [-1,1]^{m-1 \times n}\times[-1,1]^{m \times n-1}$
\begin{align*}
P_\mathcal{P}(p,q) &= (r,s) \text{ with } \left\{ \begin{aligned}
r_{ij} &= sgn(p_{ij} ) \min \{ 1, |p_{ij}| \}\\
s_{ij} &= sgn(q_{ij} ) \min \{ 1, |q_{ij}| \}\\
\end{aligned}\right.
\end{align*}
\end{exampleblock}

\begin{exampleblock}{Projected Gradient Step}
\begin{align*}
(p^{k+1},q^{k+1} ) &= P_\mathcal{P} \left((p^k, q^k) + \frac{1}{8\lambda} \mathcal{L}^TP_{[0,1]} (b - \lambda \mathcal{L}(p,q)) \right)
\end{align*}
\end{exampleblock}

\end{frame}

\begin{frame}{Monotone FISTA TV Regularization}
\begin{spacing}{1.6}
\fontsize{9}{10}\selectfont
\hspace{-3 mm}
\begin{minipage}{0.4\textwidth}
\textbf{MFISTA$(b,f, \lambda)$}
\begin{algorithmic}
\State $y^1 = x^0 = b; \, t^1 = 1$
\State $\alpha \geq Lip(\nabla f)$
\For {$k = 1:N$}
	\State $u^k = y^k - \frac{A^T\nabla f (A y^k - b)}{\alpha}$
	\State $z^k = FGP(u^k, \frac{\lambda}{2 \alpha})$
	\State $x^k = \underset{x \in \{x^{k-1},z^k\}}{\text{argmin}} \,L_b(x)$
	\State $t^{k+1} = \frac{1 + \sqrt{1 + 4{t^k}^2}}{2}$ 
	\State $y^{k+1} = x^k + \frac{t^k}{t^{k+1}}(z^k - x^k)$ 
	\State \hspace{10 mm}$+ \frac{t^{k-1}}{t^{k+1}}(z^k - x^k) $
\EndFor\\
\Return $x^N$
\end{algorithmic}
\end{minipage}
\begin{minipage}{0.6\textwidth}

\textbf{FGP$(b, \lambda)$}
\begin{algorithmic}
\State $(r_{ij}^1, s_{ij}^1) = (p_{ij}^0, q_{ij}^0) = 0; \, t^1 = 1$
\For {$k = 1:N$}
	\State $(p^k,q^k) = P_\mathcal{P} \left( (r^k,s^k) - \frac{\mathcal{L}^TP_{[0,1]} (b - \lambda \mathcal{L}(r^k,s^k))}{8\lambda} \right)$
	\State $t^{k+1} = \frac{1 + \sqrt{1 + 4{t^k}^2}}{2}$ 
	\State $(r^k,s^k) = (p^k,q^k) + \frac{t^k-1}{t^{k+1}}(p^k-p^{k-1}, q^k-q^{k-1})$ 
\EndFor \\
\Return $P_{[0,1]} (b - \lambda \mathcal{L}(p^N,q^N))$
\end{algorithmic}

\end{minipage}
\end{spacing}
\end{frame}

\begin{frame}{Results: Gaussian Noise}
\begin{center}
\vspace{-3 mm}
\includegraphics[width = 0.8\textwidth]{../figures/gaussian_peppers.png} 
\end{center}

\begin{textblock}{5}(1.5,3.6)
\rotatebox{90}{Original Image}
\end{textblock}

\begin{textblock}{5}(1.5,9.6)
\rotatebox{90}{Frobenius Loss}
\end{textblock}

\begin{textblock}{5}(14.1,3.2)
\rotatebox{90}{Blurred and Noisy}
\end{textblock}

\begin{textblock}{5}(14.1,10)
\rotatebox{90}{Huber Loss}
\end{textblock}

\end{frame}

\begin{frame}{Results: Student's t Noise}
\begin{center}
\vspace{-3 mm}
\includegraphics[width = 0.8\textwidth]{../figures/student-t_peppers.png} 
\end{center}

\begin{textblock}{5}(1.5,3.6)
\rotatebox{90}{Original Image}
\end{textblock}

\begin{textblock}{5}(1.5,9.6)
\rotatebox{90}{Frobenius Loss}
\end{textblock}

\begin{textblock}{5}(14.1,3.2)
\rotatebox{90}{Blurred and Noisy}
\end{textblock}

\begin{textblock}{5}(14.1,10)
\rotatebox{90}{Huber Loss}
\end{textblock}

\end{frame}

\section{Discussion}

\begin{frame}{Conclusions}

\begin{block}{Wavelet vs. Total Variation}
\begin{itemize}
\item Total variation seems to do better for high amounts of noise.
\item \textbf{How do they compare on timing????  Any advantages of Wavelet???}
\end{itemize}
\end{block}

\begin{block}{Frobenius vs Huber}
\begin{itemize}
\item On Gaussian noise, the two are comparable.
\item Huber outperforms Frobenius on noise with heavier tail.
\end{itemize}
\end{block}

\begin{block}{Challenges}
\begin{itemize}
\item Ideal parameter values change image to image.
\item How can we quantitatively evaluate performance?
\item How can we optimize parameters without performance metric?
\end{itemize}
\end{block}

\end{frame}

\section*{References}
\begin{frame}{\Huge Questions?}
\begin{thebibliography}{10}    
%\setbeamertemplate{bibliography item}[online]
%\bibitem{code} Codes used to generate figures
%\url{https://github.com/snagcliffs/Amath575project}
\beamertemplatebookbibitems % This gives it a nice book symbol
\bibitem{book} J. G. Nagy, P. C. Hansen, and D. P. O'Leary. \textit{Deblurring Images: Matrices, Spectra, and Filtering.} SIAM, Philadelphia, 2006. 

\beamertemplatearticlebibitems % and an article symbol
\bibitem{Wfista} A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problem. \textit{SIAM Journal of Imaging Sciences,} 2(1):183-202, 2009.

\bibitem{TVfista}  A. Beck and M. Teboulle. Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems. \textit{IEEE Transactions on Image Proccessing,} 18(11):2419-2434, 2009. 

\end{thebibliography}
\end{frame}

\end{document}
