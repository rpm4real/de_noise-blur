\documentclass[10pt,a4paper]{article}
\usepackage{nips15submit_e}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
%\usepackage[
%backend=bibtex,
%sorting=unsrt
%]{biblatex}
%\addbibresource{projectsources.bib}
\usepackage{xcolor}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{setspace}
\usepackage{framed}

\usepackage[paperwidth=8.5in,paperheight=11in,centering,hmargin=1in,vmargin=1in]{geometry}
\nipsfinalcopy

%\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\usepackage{physics}
\newcommand{\prox}{\mathrm{prox}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Tau}{\mathcal{T}}
\newcommand{\AMz}{\underset{z}{arg\, min}}

\begin{document}
	\title{Image Deblurring and Denoising \\with Various Fidelity Terms and Regularizers}
	\author{
		Kelsey Maass, ~Samuel Rudy, ~Kevin Mueller, ~and Riley Molloy\\
		University of Washington\\
	}
	
	\maketitle
	
	\begin{center}
		\begin{minipage}{0.8\textwidth}
			\begin{center}
				\textbf{Abstract}
			\end{center}
			Image deblurring and denoising is often formulated as a regularized least-squares problem. In this project, we compare the performance of two 
			different regularizers, the $l_1$ wavelet regularization and total variation regularization used by Beck and Teboulle in \cite{FISTA} and \cite{TV}. 
			We also experiment with different robust penalty functions, such as the Huber loss, in order to restore images with noise drawn from heavy-tailed 
			distributions. Results are obtained using variants of the FISTA algorithm. 
		\end{minipage}
	\end{center}
	
	% Introduction
	\section{Introduction}
	Image denoising and deblurring problems can be modeled with the linear system
	\begin{equation}
	Ax + w = b,
	\end{equation}
	where $b$ is the observed image, $w$ is some unknown noise, and $x$ is the true image we would like to recover. For a blurred image we let $A$ represent the blurring process, and for noisy images we let $A$ be the identity matrix. For many inverse problem applications the least squares approximation, 
	\begin{equation}
	\hat{x}_{LS} = \arg\min_x \| Ax - b \|_F^2 ,
	\end{equation}
	gives a reasonable solution. Unfortunately, this approach doesn't provide any new information for the denoising problem, and for image deblurring problems $A$ is often ill-conditioned, so the solution is contaminated by round-off error and amplified noise \cite{DeblurBook}. To illustrate this, we consider the pepper image in Figure 1. If we blur the image and compute the naive solution $x = A^{-1}b$, the result is obscured by round-off error in the form of ringing artifacts. The result is even worse when we add noise.
	
	In order to stabilize the solution, a variety of regularizers can be used which exploit features of the true image such as smoothness or sparsity in a wavelet domain. This results in the problem formulation
	\begin{equation}
	\hat{x} = \arg\min_{x} \| Ax - b \|_F^2 + \lambda R(x),
	\end{equation}
	where the parameter $\lambda > 0$ is chosen to balance the tradeoff between fidelity to the model and the assumed feature. In this paper we compare two choices for the regularization term: $l_1$ wavelet regularization with $R(x) = \| Wx \|_1$, and total variation regularization with $R(x) = TV(x)$, which are explored by Beck and Teboulle in \cite{TV} and \cite{FISTA} respectively.
	
	It is also known that the quadratic penalty is extremely sensitive to outliers. While this does not pose problems for images with Gaussian noise, it becomes an issues for images with noise from heavy-tailed distributions, such as the Student's $t$-distribution, which could result from a dirty camera lens. Therefore we also consider different choices for the fidelity term which are more robust to outliers, including the Huber norm
	\begin{equation} \label{huber}
	h_{\gamma}(x) = \min_y \frac{1}{2} \| x - y \|_F^2 + \gamma \| y \|_1,
	\end{equation}
	and the function
	\begin{equation} \label{log_cosh}
	g_{\gamma}(x) = \frac{1}{\gamma} \sum_i \log\left(\cosh\left(\gamma x_i\right)\right).
	\end{equation}
	
For the sake of completeness we also wanted to experiment with the 1-norm penalty but were not able to implement it due to the lack of smoothness.  Instead, we tried using a smooth approximation to the 1-norm.  The function we came up with was $\gamma^{-1} \log( \cosh(\gamma x))$ seen in Figure 2 with $\gamma = 5$, which uniformly converges to the absolute value function as $\gamma$ tends towards positive infinity.  Further, the Lipschitz constant of the gradient only increases linearly with $\gamma$.  The 1-norm could also have been approximated by the Huber loss via taking $\gamma \to 0$ while simultaneously adjusting $\lambda$ to effectively increase the slope of the fidelity function.  This presents other challenges however.  For instance it is highly nontrivial to tune the parameters of the algorithm and doing so while also worrying about taking two limits would further complicate an already challenging problem.  The behavior of $\gamma^{-1} \log( \cosh(\gamma x))$ away from the origin does not significantly change with $\gamma$ and hence parameter tuning for the near $l_1$ case is simpler than with Huber.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=2in]{../figures/fig1}
		\includegraphics[width=2in]{../figures/fig2}
		\includegraphics[width=2in]{../figures/fig3} \\[1ex]
		\includegraphics[width=2in]{../figures/fig1}
		\includegraphics[width=2in]{../figures/fig4}
		\includegraphics[width=2in]{../figures/fig5}
		\caption{\small In the top row we observe the naive solution applied to a blurred image, including the true image $x$ (left), the blurred image $b$ (middle), and the recovered image $A^{-1}b$. On the bottom row, we see the effects of adding noise illustrated by the true image $x$ (left), the blurred and noisy image $b-w$ (middle), and the recovered image $A^{-1}(b-w)$.}
	\end{figure}
	
	\begin{figure}[H]\label{penalties}
		\centering
		\includegraphics[scale=.35]{../figures/penalty_functions.png} 
		\caption{The various penality functions we utilize in our fidelity term, and their comparision to the $l_1$ norm.}
	\end{figure}
	
	% Problem Formulation
	\newpage
	\section{Problem Formulation}
	The general approach to the image deblurring and denoising problem can now be stated as a minimization problem of the form
	\begin{equation} \label{general}
	\min_x f(Ax -b) + \lambda R(x),
	\end{equation}
	where $Ax$ is a discrete convolution of the true image with a blur kernel, the  \emph{fidelity term} $f(Ax-b)$ measures how well the recovered image complies with the linear model, and $R(x)$ is our chosen \emph{regularization}. Furthermore, if we assume that the pixels of our image satisfy $x_{i,j} \in [0,1]$, we can add an additional term to the objective function
	\begin{equation} \label{loss}
	L_b(x) = f(Ax-b) + \lambda R(x) + \delta(x | [0,1] ),
	\end{equation}
	where $\delta(x | [0,1])$ is the indicator function for the set $[0,1]$. 
	
	% Blur operators
	\subsection{Blur Operators}
	The blurring matrix $A$ is built from two ingredients: the blur kernel, which specifies how each pixel spreads out in the blurred image, and the boundary conditions, which specify our assumptions about the scene just outside of our observed image. For our examples, we use a $9 \times 9$ Gaussian blur kernel with a standard deviation of one, depicted in Figure 3. The Gaussian kernel is popular in blurring applications, and can be used to simulate atmospheric turbulence \cite{DeblurBook}. 
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.5in]{../figures/pixel} \hspace{2em}
		\includegraphics[width=1.5in]{../figures/psf}
		\caption{A single pixel before blurring (left) and after blurring (right). The image on the right is called the \emph{point spread function}, or the blur kernel. In this 	
		example the blur kernel is computed by generating a $9 \times 9$ Gaussian with standard deviation one centered about the pixel and normalized so that all
		elements sum to one.  }
	\end{figure}
	
	In order to blur or deblur pixels near the border of our image, we need to decide how to represent the pixels just outside of our image boundaries, which we do not have. If we assume that these pixels mirror those inside the image, then we impose reflexive boundary conditions. For spatially invariant and doubly symmetric blur kernels, these boundary conditions result in a blurring matrix that can be diagonalized by the orthogonal two-dimensional discrete cosine transform
	\begin{equation}
	A = C^T \Lambda C,
	\end{equation}
	where the eigenvalues of $A$ are determined by the blur kernel. There are two advantages of representing the blur operator in this way. First, we do not have to construct the matrix explicitly. Instead, we can take advantage of MATLAB's efficient discrete cosine transform function to compute our blurred image:
	\begin{equation}
	Ax = C^T \Lambda C x = {\tt idct2}( \Lambda {\tt dct2}(x)) = b.
	\end{equation}
	Second, when our fidelity function is given by $f(Ax -b) = \| Ax - b \|_F^2$, we can exploit the fact that the Frobenius norm is orthogonally invariant and $C$ is isometric to rewrite the fidelity term as
	\begin{align*}
	f(Ax - b) &= \| Ax - b \|_F^2 \\
	&= \| C^T \Lambda C x - b \|_F^2 \\
	&= \| \Lambda Cx - Cb \|_F^2 \\
	&= \| \Lambda \hat{x} - \hat{b} \|_F^2.
	\end{align*}
	In this case we take the Frobenius form of our transformed images $\hat x$ and $\hat b$, multiplying $\hat x$ by the diagonal eigenvalue matrix rather than the full blur matrix. We can also compute and store $\hat b$ for additional efficiency. 
	
	% 1-Norm Wavelet Regularization
	\subsection{$l_1$ Wavelet Regularization}
	
	The wavelet regularization deblurring model, as seen in \cite{FISTA}, can be written in the form of \eqref{general} as 
	\begin{equation} \label{wavelet_orig}
	\min_{x} f(Ax-b) + \lambda \norm{Wx}_1 ,
	\end{equation}
	where $W$ corresponds to a given wavelet transform and $\|Wx\|_1$ is the sum of absolute values of all entries of the matrix $Wx$. This regularization is motivated by the fact that natural images often have a sparse representation in wavelet domains, which we can encourage in our recovered image using the $l_1$ norm. 
	
	To illustrate this fact, we look at the orthogonal two-dimensional Haar wavelet transform of the peppers image. First we transform the image using the spot operator {\tt opHaar2} with one level. This results in an image where most of the elements are zero or near zero, except for those in the upper left quadrant. If we repeat this process four more times, taking the transform of the resulting corner image, we get the Haar transform of the image for five levels, which is even more sparse. 
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=2.5in]{../figures/haarTrans2} \quad
		\includegraphics[width=2.5in]{../figures/haarTrans3}
		\caption{The 2D Haar wavelet transform of the peppers image using one level (left) and five levels (right). Note that the images are sparse. }
	\end{figure}
	
	The Haar wavelet is just the first of many wavelets developed for multi-resolution analysis. In addition to the orthogonal two-dimensional Haar wavelet transform, we therefore also consider the orthogonal two-dimensional Daubechies wavelet transform implemented with the spot operator {\tt opWavelet2}. 
	
	% TV Regularization
	\subsection{Total Variation Regularization}
	The total variation deblurring model, as seen in \cite{TV}, can be written in the form of \eqref{general} as 
	\begin{equation} \label{tv_orig}
	\min_{x} f(Ax-b) +  \lambda \mathrm{TV}(x)+ \delta(x | [0,1]),
	\end{equation}
	where $\mathrm{TV}(x)$ is the total variation semi-norm.  We include the indicator function to ensure that the resulting image is within the correct range of pixel values.  Two choices exist for the TV-norm: the isotropic version and the $l_1$-based, anisotropic version. Since both methods yield very similar results, in this project we work exclusively with the $l_1$-based TV-norm, defined as 
	$$ TV_{l_1}(x) = \sum_{i=1}^{m-1} \sum_{j=1}^{n-1} \left( \abs{x_{i,j}  - x_{i+1,j} } + \abs{ x_{i,j} - x_{i,j+1}  } \right) + \sum_{i=1}^{m-1} \abs{ x_{i,n} - x_{i+1,n} } + \sum_{j=1}^{n-1} \abs{ x_{m,j} - x_{m,j+1 } },$$
	for $x \in \R^{m \times n},$ where reflexive boundary conditions
	\begin{align*}
	x_{m+1,j} - x_{m,j} &= 0, \textrm{ for all }j \\
	x_{i,n+1} - x_{i,n} &= 0, \textrm{ for all }i
	\end{align*}
	are assumed. 
	
	The motivation behind the TV regularization is that images are often ``smooth", or that most pixels will have values similar to their neighbors. Alternatively, it assumes that high frequency content such as noise increases the total variation of an image, defined as the absolute value of the gradient. To illustrate this, we look at the absolute value of the row and column differences of the peppers image. Here we see that most of the values are indeed small, except for those which represent edges. 
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=2.5in]{../figures/diff1} \quad
		\includegraphics[width=2.5in]{../figures/diff2}
		\caption{The absolute value of the column (left) and row (left) differences of the peppers image. Note that most of the values are zero or near zero, with most of the variation present in the image edges. }
	\end{figure}
	
	% Algorithms
	\section{Algorithms}
	In order to solve our deblurring and denoising problem, where our objective function is the sum of two convex functions (one smooth and one with a convenient proximal representation), we can employ the proximal gradient method. For each iteration we take a gradient step in $f$ and apply the proximal operator of $R$,
	\begin{equation}
	\begin{aligned}
	x_{k+1} &= \prox_{\alpha^{-1}\lambda R}\big(x_k - \alpha^{-1} A^T \nabla f(Ax_k-b)\big) \\
	&= \min_y \frac{1}{2} \Big\| y - \big(x_k - \alpha^{-1} A^T \nabla f(Ax_k-b) \big)\Big\|_2^2 + \alpha^{-1}\lambda R(y)
	\end{aligned}
	\end{equation}
	
	where $\alpha \geq Lip(\nabla f)$. For faster convergence, we use the modified version of the proximal gradient algorithm, FISTA, which utilizes information from previous iterations:
	\begin{equation}
	\begin{aligned}
	x_{k} &= \prox_{\alpha^{-1}\lambda R}\big(y_k - \alpha^{-1} A^T \nabla f(Ay_k-b)\big) \\[1ex]
	t_{k+1} &= \frac{1+\sqrt{1+4t_k^2}}{2} \\
	y_{k+1} &= x_k + \left( \frac{t_k-1}{t_{k+1}} \right) (x_k - x_{k-1})
	\end{aligned}
	\end{equation}
	
	We detail the implementation of FISTA for each choice of regularization term in the two sections that follow.
	
	% Wavelet algorithm
	\subsection{$l_1$ Wavelet Regularization}
	
	Because the wavelet transform $W$ is an orthogonal operator, we can evaluate the proximal operator of $R(x) = \|Wx\|_1$ via the soft-thresholding operation:
	\begin{equation}
	\begin{aligned}
	\prox_{\alpha^{-1}\lambda \| W \cdot \|_1}(x) &= \arg\min_y \frac{1}{2} \| y-x \|_2^2 + \alpha^{-1}\lambda \| Wy \|_1 \\
	&= \arg\min_y \frac{1}{2} \| Wy - Wx \|_2^2 + \alpha^{-1}\lambda \|Wy \|_1 \\
	&= W^T \arg\min_{Wy} \frac{1}{2} \| Wy - Wx \|_2^2 + \alpha^{-1}\lambda \|Wy \|_1 \\
	&= W^T \prox_{\alpha^{-1}\lambda \|\cdot\|_1}(Wx) \\
	&= W^T \text{sgn}(Wx) \max(0, |Wx|-\alpha^{-1}\lambda)
	\end{aligned}
	\end{equation}
	
	\newpage
	To deblur and denoise our images using the $l_1$ wavelet regularization and either the Frobenius or Huber norm fidelity term, we therefore employ the following algorithm:
	
	\begin{center}
		\begin{minipage}[T]{0.5\textwidth}
			\begin{framed}
				\textbf{FISTA$(b,f,\lambda)$}
				\begin{algorithmic}
					\State $y_1 = x_0 = b; \, t_1 = 1$
					\State $\alpha \geq Lip(\nabla f)$
					\For {$k = 1:N$}
					\State $u_k = y_k - \alpha^{-1} A^T \nabla f(Ay_k-b)$ \vspace{1ex}
					\State $x_k = W^T\text{sgn}(Wu_k)\max(0,|Wu_k|-\alpha^{-1}\lambda)$ \vspace{1ex}
					\State $t_{k+1} = \frac{1+\sqrt{1+4t_k^2}}{2}$
					\State $y_{k+1} = x_k + \left(\frac{t_k-1}{t_{k+1}}\right) (x_k - x_{k-1})$
					\EndFor\\
					\Return $x_N$
				\end{algorithmic}
			\end{framed}
		\end{minipage}
	\end{center}
	
	% TV algorithm
	\subsection{Total Variation Regularization}
	
	Optimization of the objective function \eqref{tv_orig} is based on the proximal gradient algorithm and exploits the idea of momentum used in FISTA as well as an extra function evaluation at each iteration in order to ensure a non-increasing objective function.  The proximal gradient step is given as follows,
	\begin{align*}
	x_{k+1} &= \prox_{\alpha^{-1}(\lambda \textrm{TV}(\cdot) + \delta_{[0,1]})} (\underbrace{x_k - \alpha^{-1} A^T\nabla f (Ax_k - b)}_{u_k}) \\
	&= \arg \min_z \left( \|u_k - z\|_F^2 + \alpha^{-1}\lambda TV(z) + \delta(z | [0,1]) \right) \\
	&= P_{[0,1]}  \left( \arg \min_z \left( \|u_k - z\|_F^2 + \alpha^{-1}\lambda TV(z) \right) \right)
	\end{align*}
	Here the learning rate $\alpha$ is taken to be no larger than than the multiplicative inverse of the Lipschitz constant of $\nabla f$.  Note that in order to evaluate this expression we must solve a denoising problem with input $u_k = x_k - \alpha^{-1} A^T\nabla f (Ax_k - b)$.  Of particular note is that regardless of the original fidelity term we have a Frobenius norm fidelity term in the new denoising problem.  This allows for the easy manipulation of the original fidelity function, so long as it is convex and has a Lipschitz gradient.
	
	The resulting denoising problem with Frobenius norm fidelity function may be solved rapidly using the method developed in \cite{TV}.  The $TV$ function is shown to have a dual representation as a trace and the relationship between trace and Frobenius norm is heavily exploited to develop a dual formulation of the denoising problem.  We present their method here, starting with a few new definitions which will be necessary.
	\begin{align*}
	\mathcal{P} &= \{ (p,q) \in \R^{(m-1) \times n} \times \R^{ m \times (n-1) } :  \abs{p_{i,j} } \le 1, \abs{p_{ i,j } } \le 1 \} \\
	\mathcal{L} &: \R^{(m-1) \times n} \times \R^{ m \times (n-1) } \rightarrow \R^{m \times n}  \text{ such that } \mathcal{L}(p,q)_{i,j} = p_{i,j} + q_{i,j} - p_{ i-1,j } - q_{ i,j-1 }\\ \hspace{3 cm}& \text{ for } i = 1,\dots,m,\,j = 1,\dots,n \text{ and }  p_{0,j} = p_{m,j} = q_{i,0} = q_{i,n} = 0
	\end{align*}
	Using these new definitions we claim without proof that the total variation functional may be written as
	\begin{equation}
	TV(x) = \underset{(p,q) \in \mathcal{P}}{\max} \Tr(\mathcal{L}(p,q)^Tx),
	\end{equation}
	and we may now exploit the relationship between trace and Frobenius norm to obtain the dual problem for Frobenius denoising using total variation regularization.  \\
	\begin{align*}
	\min_{x \in [0,1]} \norm{x-b}_F^2 +2 \lambda \mathrm{TV}(x) &= \min_{x \in [0,1]}  \underset{(p,q) \in \mathcal{P}}{\max}  \norm{x-b}_F^2 + 2\lambda \Tr(\mathcal{L}(p,q)^Tx) \\
	&= \underset{(p,q) \in \mathcal{P}}{\max}   \min_{x \in [0,1]} \norm{x-b}_F^2 +2 \lambda \Tr(\mathcal{L}(p,q)^Tx) \\
	&=\underset{(p,q) \in \mathcal{P}}{\max}   \min_{x \in [0,1]} \norm{x}_F^2 + \norm{b}_F^2 - 2 \Tr(x^Tb)+ 2\lambda \Tr(\mathcal{L}(p,q)^Tx) \\
	&=\underset{(p,q) \in \mathcal{P}}{\max}   \min_{x \in [0,1]} \norm{x}_F^2 + \norm{b}_F^2 - 2 \Tr(x^T(b- \lambda\mathcal{L}(p,q)))\\
	&=\underset{(p,q) \in \mathcal{P}}{\max}   \min_{x \in [0,1]} \underbrace{\norm{x}_F^2 + \norm{b- \lambda\mathcal{L}(p,q)}_F^2 - 2 \Tr(x^T(b- \lambda\mathcal{L}(p,q)))}_{ = \norm{x-(b- \lambda\mathcal{L}(p,q))}_F^2} -  \norm{b- \lambda\mathcal{L}(p,q)}_F^2+ \norm{b}_F^2\\
	&= \underset{(p,q) \in \mathcal{P}}{\max}   \min_{x \in [0,1]}  \norm{x-(b- \lambda\mathcal{L}(p,q))}_F^2 - \norm{b- \lambda\mathcal{L}(p,q)}_F^2+ \norm{b}_F^2 \\
	\end{align*}
	Here the switching of min and max is permitted due to the convexity of the objective in $x$ and concavity in $p$ and $q$ \cite{TV}.  Note that the problem of maximizing over $x$ is simply the projection of each index of  $b- \lambda\mathcal{L}(p,q)$ onto the set $[0,1]$.  This gives the optimality condition for $x$ in terms of $p$ and $q$ which we plug back in to obtain the dual problem.
	
	\begin{align*}
	(p^*,q^*) &= \underset{(p,q) \in \mathcal{P}}{\text{arg }\max}  \norm{P_{[0,1]}(b- \lambda\mathcal{L}(p,q)) -(b- \lambda\mathcal{L}(p,q))}_F^2 - \norm{b- \lambda\mathcal{L}(p,q)}_F^2+ \norm{b}_F^2\\
	x &= P_{[0,1]}(b- \lambda\mathcal{L}(p^*,q^*)) \\
	\end{align*}
	Note that in the dual problem the objective is Lipschitz differentiable.  We claim without proof that the gradient has Lipschitz constant $L \leq 16 \lambda^2$ and refer the reader to \cite{TV} for proof.  This smoothness allows us to implement a projected gradient algorithm.
	
	We implement the above optimization scheme by using a slightly optimized method referred to as Monotone FISTA or MFISTA. In FISTA, the objective function values are not guaranteed to be nonincreasing, so MFISTA requires the evaluation of the objective function to check for monotonicity.  Convergence plots shown in \cite{TV} indicate that MFISTA offers substantial improvements for images with substantial amounts of noise.  We also implement a sped-up projected gradient algorithm using the same momentum technique as in FISTA.  This algorithm is labeled as FGP, for fast gradient projection. \\
	\begin{framed}
		\begin{minipage}{0.48\textwidth}
			\textbf{MFISTA$(b,f, \lambda)$}
			\begin{algorithmic}
				\State $y_1 = x_0 = b; \, t_1 = 1$
				\State $\alpha \geq Lip(\nabla f)$
				\For {$k = 1:N$}
				\State $u_k = y^k - \frac{A^T\nabla f (A y_k - b)}{\alpha}$
				\State $z_k = FGP(u_k, \frac{\lambda}{2 \alpha})$
				\State $x_k = \underset{x \in \{x_{k-1},z_k\}}{\text{argmin}} \,L_b(x)$
				\State $t_{k+1} = \frac{1 + \sqrt{1 + 4t_k^2}}{2}$ 
				\State $y_{k+1} = x^k + \frac{t_k}{t_{k+1}}(z_k - x_k)$ 
				\State \hspace{10 mm}$+ \frac{t_{k-1}}{t_{k+1}}(z_k - x_k) $
				\EndFor\\
				\Return $x_N$
			\end{algorithmic}
		\end{minipage}
		\begin{minipage}{0.48\textwidth}
			
			\textbf{FGP$(b, \lambda)$}
			\begin{algorithmic}
				\State $(r_{ij}^1, s_{ij}^1) = (p_{ij}^0, q_{ij}^0) = 0; \, t_1 = 1$
				\For {$k = 1:N$}
				\State $(p_k,q_k) = P_\mathcal{P} \left( (r_k,s_k) - \frac{\mathcal{L}^TP_{[0,1]} (b - \lambda \mathcal{L}(r_k,s_k))}{8\lambda} \right)$
				\State $t_{k+1} = \frac{1 + \sqrt{1 + 4t_k^2}}{2}$ 
				\State $(r_k,s_k) = (p_k,q_k) + \frac{t_k-1}{t_{k+1}}(p_k-p_{k-1}, q_k-q_{k-1})$ 
				\EndFor \\
				\Return $P_{[0,1]} (b - \lambda \mathcal{L}(p_N,q_N))$
			\end{algorithmic}
			
		\end{minipage}
	\end{framed}
	
	% Choosing parameters
	\newpage
	\subsection{Parameter Selection}
	Before we can restore a corrupted image, we must first set our objective function parameters $\lambda$ and $\gamma$. For our results in the next section we tried various parameter values and chose those that resulted in the smallest error from the true image, $\| x - \hat x \|_F$, which seemed to correspond to the best looking image. Unfortunately, in most applications the true image is not known, so parameters must be chosen by visual inspection. This is demonstrated in Figure 6 below, where we vary $\lambda$ while we deblur and denoise the image with Student's $t$ noise with one degree of freedom and a standard deviation of 0.0001 and the Frobenius norm penalty function. 
	
	We see that smaller values of $\lambda$ result in underregularization: the image is successfully deblurred, but the sensitivity of the Frobenius norm to outliers results in star-like artifacts. As we increase $\lambda$ these artifacts diminish, but we pay the price in terms of overregularization. For the Haar wavelet basis this results in pixelation, while for the Daubechies wavelet basis the image appears blurred or smudged. It is clear from looking at the images that $\lambda = 0.01$ does the best job at balancing fidelity and regularization, in fact returning the lowest error. 
	
	Unfortunately, the final objective function value does not seem to be a good indicator for image quality, as it increases with the amount of regularization applied regardless of how well the image is actually restored. Therefore hand-tuning the parameters seems to be the only choice when the true image is not known.  \\
	
	\begin{minipage}{0.5\textwidth}
	\begin{center}Error: $\| x - \hat x \|_F$\end{center}
	\begin{tabular}{|c|c|c|c|c|}
	\hline 
	& $\lambda$ = 1e-4 & $\lambda$ = 1e-3 & $\lambda$ = 1e-2 & $\lambda$ = 1e-1 \\
	\hline
	H &50.4436   &22.2812   &10.8628   &15.8965 \\
	D &50.0977   &20.8619    &7.6839   &12.9139 \\
	\hline
	\end{tabular}
	\end{minipage}
	\begin{minipage}{0.5\textwidth}
	\begin{center}Objective Function: $\frac{1}{2}\|Ax-b\|_F^2+\lambda \|Wx\|_1$\end{center}
	\begin{tabular}{|c|c|c|c|c|}
	\hline
	&$\lambda$ = 1e-4 & $\lambda$ = 1e-3 & $\lambda$ = 1e-2 & $\lambda$ = 1e-1 \\
	\hline
	H &3.0954    &9.3127   &52.2841  &379.5130 \\
	D &3.0598    &8.8598   &48.0596  &350.2697 \\
	\hline
	\end{tabular}
	\end{minipage}
	
	\begin{figure}[h]
	\centering
	\rotatebox{90}{\hspace{0.5cm} Daubechies \hspace{3.5em} Haar}
	\includegraphics[width=6in]{../figures/wavLam4.pdf} \\
	$\lambda = 0.0001$ \hspace{2cm} $\lambda = 0.001$ \hspace{2.25cm} $\lambda = 0.01$ \hspace{2.25cm} $\lambda = 0.1$
	\caption{Results obtained from debluring and denoising the peppers image with Student's $t$ noise, the Frobenius norm fidelity function, and $l_1$ wavelet regularization. As we vary $\lambda$, we note how under- and overregularization affects image quality for our two wavelet bases.}
	\label{lambda_comp}
	\end{figure}
	
	% Examples
	\section{Examples}
	
	In this section we compare the performance of our different fidelity term penalty functions and regularizers. In each example the peppers image is first blurred with a $9 \times 9$ Gaussian kernel with standard deviation 1, and then either Gaussian noise or Student's $t$ noise is added, both with standard deviation 0.01. Additionally, we determined how many iterations to run each algorithm based on convergence criterion rather than using a fixed number of iterations. For our examples, we stopped iterating once the objective function failed to decrease by more than 0.1\% of its previous value.  When testing the total variation algorithm the objective function often remained constant due to the structure of the MFISTA algorithm so the break condition was restricted to the case in which decay of the objective function was both sufficiently small and non-zero.
	
%	\subsection{$l_1$ Wavelet Regularization}
%	
%
%	
%	DISCUSS WHICH REGULARIZER IS BETTER, WHICH NORM IS BETTER
	
	% Wavelet Gauss
	\subsection{$l_1$ Wavelet Regularization: Gaussian Noise}
	\begin{figure}[H]
		\begin{center}
			\rotatebox{90}{\hspace{1cm} Frobenius Loss \hspace{2.25cm} Original Image}
			\includegraphics[width = 0.75\textwidth]{../figures/waveletGaussH.pdf} 
			\rotatebox{90}{\hspace{1.5cm} Huber Loss \hspace{2cm} Blurred and Noisy}
		\end{center}
		\caption{Images with Gaussian noise restored with the $l_1$ Haar wavelet regularizer. The Frobenius loss formulation uses $\lambda = 0.001$, while the Huber loss formulation uses $\lambda = 0.001$ and $\gamma = 0.01$. }
		\label{waveletH_gauss}
	\end{figure}
	\vspace{-1ex}
	\begin{figure}[H]
		\begin{center}
			\rotatebox{90}{\hspace{1cm} Frobenius Loss \hspace{2.25cm} Original Image}
			\includegraphics[width = 0.75\textwidth]{../figures/waveletGaussD.pdf} 
			\rotatebox{90}{\hspace{1.5cm} Huber Loss \hspace{2cm} Blurred and Noisy}
		\end{center}
		\caption{Images with Gaussian noise restored with the $l_1$ Daubechies wavelet regularizer. The Frobenius loss formulation uses $\lambda = 0.1$, while the Huber loss formulation uses $\lambda = 0.01$ and $\gamma = 0.01$. }
		\label{waveletD_gauss}
	\end{figure}
	
	% Wavelet Student
	\subsection{$l_1$ Wavelet Regularization: Student's $t$ Noise}
	\begin{figure}[H]
		\begin{center}
			\rotatebox{90}{\hspace{1cm} Frobenius Loss \hspace{2.25cm} Original Image}
			\includegraphics[width = 0.75\textwidth]{../figures/waveletStudentH.pdf} 
			\rotatebox{90}{\hspace{1.5cm} Huber Loss \hspace{2cm} Blurred and Noisy}
		\end{center}
		\caption{Images with Student's $t$ noise restored with the $l_1$ Haar wavelet regularizer. The Frobenius loss formulation uses $\lambda = 0.01$, while the Huber loss formulation uses $\lambda = 0.001$ and $\gamma = 0.01$.}
		\label{waveletH_student}
	\end{figure}
	\vspace{-1ex}
	\begin{figure}[H]
		\begin{center}
			\rotatebox{90}{\hspace{1cm} Frobenius Loss \hspace{2.25cm} Original Image}
			\includegraphics[width = 0.75\textwidth]{../figures/waveletStudentD.pdf} 
			\rotatebox{90}{\hspace{1.5cm} Huber Loss \hspace{2cm} Blurred and Noisy}
		\end{center}
		\caption{Images with Student's $t$ noise restored with the $l_1$ Daubechies wavelet regularizer. The Frobenius loss formulation uses $\lambda = 0.1$, while the Huber loss formulation uses $\lambda = 0.01$ and $\gamma = 0.01$.}
		\label{waveletD_student}
	\end{figure}
	
	\subsection{Total Variation Regularization}
	
%	We demonstrate some results of denoising and deblurring with the Total Variation regularization on real images. As previously shown, each image was given a small amount of either Gaussian or Student's t noise and blurred. Two different fidelity functions were used: the squared Frobenius norm, and the Huber norm. Figure \ref{tv_gauss} demonstrates the results using Gaussian noise, and Figure \ref{tv_student} demonstrates the results using Student's t noise with one degree of freedom. For the Gaussian noise, we set $\lambda = .001$, and $\gamma = .02$ (see TV-regularization in Problem Formulation). For Student's t noise, we set $\lambda = .002$, and $\gamma = .02$.
	
	\begin{figure}[H]
		\begin{center}
			\rotatebox{90}{\hspace{1cm} Frobenius Loss \hspace{2.25cm} Original Image}
			\includegraphics[width = 0.75\textwidth]{../figures/gaussian_peppers.png} 
			\rotatebox{90}{\hspace{1.5cm} Huber Loss \hspace{2cm} Blurred and Noisy}
		\end{center}
		\caption{Images with Gaussian noise restored with the total variation regularization. The Frobenius loss formulation uses $\lambda = 0.001$, while the Huber loss formulation uses $\lambda = 0.001$ and $\gamma = 0.02$.}
		\label{tv_gauss}
	\end{figure}
	\vspace{-1ex}
	\begin{figure}[H]
		\begin{center}
			\rotatebox{90}{\hspace{1cm} Frobenius Loss \hspace{2.25cm} Original Image}
			\includegraphics[width = 0.75\textwidth]{../figures/student-t_peppers.png} 
			\rotatebox{90}{\hspace{1.5cm} Huber Loss \hspace{2cm} Blurred and Noisy}
		\end{center}
		\caption{Images with Student's $t$ noise restored with the total variation regularization. The Frobenius loss formulation uses $\lambda = 0.002$, while the Huber loss formulation uses $\lambda = 0.002$ and $\gamma = 0.02$. }
		\label{tv_student}
	\end{figure}
	
	\subsection{The $\log \circ \cosh$ Penalty Function}
	
	Recall that for large values of $\gamma$ the function $\gamma^{-1} \log(\cosh (\gamma \cdot ))$ uniformly converges to the absolute value function.  Since it is also Lipschitz differentiable we are able to use it in our algorithm for TV regularized denoising and deblurring with high values of $\gamma$ to approximate how the standard $L^1$ norm would perform without needing to worry about how to optimize it.  Figure \ref{log_cosh_pic}
compares the effectiveness of $\log \circ \cosh$ with Huber for the denoising and deblurring problem with Student's $t$ noise.
	
	\begin{figure}[H]
		\begin{center}
			\rotatebox{90}{\hspace{1.25cm} $\log \circ \cosh$ Loss \hspace{2.5cm} Original Image}
			\includegraphics[width=0.8\textwidth]{../figures/logcosh_peppers.png}
			\rotatebox{90}{\hspace{1.5cm} Huber Loss \hspace{2.25cm} Blurred and Noisy}
			\caption{A comparison between the $\log \circ \cosh$ and Huber fidelity terms. }
			\label{log_cosh_pic}
		\end{center}
	\end{figure}
	
	% Discussion
	\section{Discussion and Conclusions}
	
	\begin{figure}[t]
		
		\begin{center}
			\includegraphics[width=0.7\textwidth]{../figures/comparePlot2.pdf}
			\caption{Frobenius norm error between the restored and true image plotted against time required for the relative change in the objective function to drop below $0.1\%$.}
			\label{comp_methods}
		\end{center}
	\end{figure}
	
	From our results, it appears that for Gaussian noise the Frobenius norm and Huber norm produce similar results, but the Huber norm outperforms the Frobenius norm when Student's $t$ noise is present. In Figures \ref{waveletH_student}, \ref{waveletD_student}, and \ref{tv_student}, we see that the images produced using the Frobenius norm fidelity term demonstrate overregularization, resulting in either pixelation, smudges, or a painted effect depending upon the regularizer. Furthermore, all of the regularizers perform fairly well for Gaussian noise, with the total variation regularizer producing the clearest image overall, and the Daubechies wavelet regularizer producing a better image than the Haar wavelet regularizer. 
	
	In Figure \ref{log_cosh_pic} we compare the $\log \circ \cosh$ penalty to the Huber with Student's $t$ noise, where it appears to perform slightly better in terms of edge resolution and noise reduction. However, their performance may be reversed for other types of noise. For example, we may expect $\log \circ \cosh$ to over-penalize low magnitude differences in the case of more tightly distributed (e.g. Gaussian) noise. Another caveat is that since the Lipschitz constant of $\nabla \gamma^{-1} \log ( \cosh ( \gamma x )) = \tanh (\gamma x)$ is equal to $\gamma$, the step size in the MFISTA algorithm decreases as we increase $\gamma$, thereby increasing the computational expense for this penalty function. Timing information was not collected for this method, but there did not appear to be a significant increase in the number of iterations needed for convergence for the examples we implemented.
		
	In order to objectively compare our different methods, we plot computation time verses the Frobenius error between the restored image and the true image in Figure \ref{comp_methods}. Here we note again that the Huber norm produces better results than the Frobenius norm, though it requires more time, with the most significant difference appearing for images with Student's $t$ noise, especially for the total variation method. It is also clear that total variation regularization outperforms the $l_1$ wavelet regularization, though at the price of additional computational complexity. This is not altogether surprising due to the fact that the total variation algorithm involves solving an additional denoising problem for each iteration of deblurring. It can also be seen that the images with Student's $t$ noise generally required less time to restore than images with Gaussian noise, though the results had slightly more error.
	
	Overall in our survey of different fidelity functions and regularizations for the deblurring and denoising problem we find that the results are greatly impacted by the amount and type of noise. For lower amounts of noise, especially Gaussian noise, all of our formulations perform well, but when higher, heavy-tailed noise is present, the total variation regularizer paired with the Huber or $\log \circ \cosh$ penalty function performs the best, though requiring more computation time. Unfortunately we did not look into how each of our algorithms scale, so this increased complexity may become more significant for larger pictures, and in that case the wavelet regularizers may be the better choice. 
	
	In our research we also came across many challenges and avenues for future work. It is clear from Figure \ref{lambda_comp} that the correct choice of the regularization parameter is important for maximizing results but varies for different classes of images. Therefore, there is a need for finding an objective method for selecting the regularization parameter across different images. Additionally, finding an appropriate error metric can be challenging when quantitatively analyzing performance for our various methods. While the Frobenius norm of the error is a common choice, it does not always accurately evaluate qualitative improvements in image restoration due to the effect of large structural changes in the image. Furthermore, in applications the true image is unknown, so evaluation must be done by visual inspection.
	
	%\printbibliography[title={Sources}]
	\bibliographystyle{unsrt}
	\bibliography{sources}
	
	{\footnotesize
	All of our code can be found on {\color{blue}\href{https://github.com/rpm4real/de_noise-blur}{github}}. \\
	The spot operators used to implement blur operators and wavelet transforms can be found {\color{blue}\href{http://www.cs.ubc.ca/labs/scl/spot/}{here}}.}
	
\end{document}